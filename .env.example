# ============================================================================
# Aura IA MCP Environment Configuration (Canonical Architecture)
# Copy this file to .env and customize for your environment
# ============================================================================

# ----------------------------------------------------------------------------
# Canonical Port Architecture (PRD Compliant)
# ----------------------------------------------------------------------------
# AURA_IA_GATEWAY_PORT=9200   # MCP SSE Gateway
# AURA_IA_ML_PORT=9201        # ML Backend
# AURA_IA_RAG_PORT=9202       # Qdrant HTTP
# AURA_IA_RAG_GRPC_PORT=9203  # Qdrant gRPC
# AURA_IA_METRICS_PORT=9204   # Prometheus (optional)
# AURA_IA_DASHBOARD_PORT=9205 # Dashboard UI
# AURA_IA_ROLE_ENGINE_PORT=9206 # ARE+ Role Engine (future)

# ----------------------------------------------------------------------------
# Core Configuration
# ----------------------------------------------------------------------------
GITHUB_TOKEN=your_github_token_here
OPENAI_API_KEY=your_openai_key_here
ANTHROPIC_API_KEY=your_anthropic_key_here
DEEPSEEK_API_KEY=your_deepseek_key_here
GROQ_API_KEY=your_groq_key_here
GOOGLE_API_KEY=your_google_key_here

# ----------------------------------------------------------------------------
# Home Assistant Integration
# ----------------------------------------------------------------------------
HA_URL=http://your-home-assistant-ip:8123
HA_FALLBACK_URL=http://your-nas-ip:8123
HA_TOKEN=your_long_lived_access_token_here

# ----------------------------------------------------------------------------
# Smart TV Integration (LG WebOS)
# ----------------------------------------------------------------------------
SMART_TV_IP=your_tv_ip_address
SMART_TV_MAC=aa:bb:cc:dd:ee:ff
SMART_TV_CLIENT_KEY=your_tv_client_key

# ----------------------------------------------------------------------------
# Media Automation (Sonarr/Radarr/SABnzbd/Plex)
# ----------------------------------------------------------------------------
SONARR_URL=http://your-nas-ip:8989
SONARR_API_KEY=your_sonarr_api_key

RADARR_URL=http://your-nas-ip:7878
RADARR_API_KEY=your_radarr_api_key

SABNZBD_URL=http://your-nas-ip:8080
SABNZBD_API_KEY=your_sabnzbd_api_key

PLEX_URL=http://your-nas-ip:32400
PLEX_TOKEN=your_plex_token

ORGANIZR_API_KEY=your_organizr_api_key

# ----------------------------------------------------------------------------
# Database Configuration
# ----------------------------------------------------------------------------
DATABASE_URL=postgresql://Admin@aura-ia-postgres:5432/aura_db

# ----------------------------------------------------------------------------
# Gateway Proxy (for Docker networking)
# ----------------------------------------------------------------------------
USE_GATEWAY_PROXY=true
GATEWAY_URL=http://aura-ia-gateway:9200

# Aura IA Service Naming (PRD Compliant)
AURA_IA_GATEWAY_HOST=aura-ia-gateway
AURA_IA_ML_HOST=aura-ia-ml
AURA_IA_RAG_HOST=aura-ia-rag
AURA_IA_DASHBOARD_HOST=aura-ia-dashboard

# Optional ULTRA flags
IDE_AGENTS_ULTRA_ENABLED=true
IDE_AGENTS_ULTRA_LOCAL=false
IDE_AGENTS_ULTRA_MOCK=false

# Optional telemetry location inside containers
MCP_TOOL_SPANS_DIR=/app/logs

# ----------------------------------------------------------------------------
# Wave 6: Advanced Retrieval Configuration (FREE LOCAL DEPLOYMENT)
# ----------------------------------------------------------------------------

# Embedding Model (Phase 1) - Models auto-download from HuggingFace (FREE)
# Options: all-MiniLM-L6-v2 (fast, 80MB), all-mpnet-base-v2 (quality, 420MB)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DEVICE=cpu  # or cuda for GPU acceleration

# ----------------------------------------------------------------------------
# Audio I/O Configuration (TTS/STT)
# ----------------------------------------------------------------------------

# Text-to-Speech (Coqui TTS) - Hybrid Model Selection
# TTS_MODEL_NAME options:
#   - "vits": 10x faster, non-autoregressive, 0.05x RTF (requires espeak-ng!)
#   - "tacotron2" (default if no espeak-ng): Works everywhere, 0.5-0.7x RTF
#   - Full path: "tts_models/en/ljspeech/vits" or any Coqui model
# TTS_USE_GPU options:
#   - "auto" (default): Detect GPU via torch.cuda.is_available()
#   - "true": Force GPU (fails if unavailable)
#   - "false": Force CPU
# NOTE: For VITS (10x faster), install espeak-ng from:
#       https://github.com/espeak-ng/espeak-ng/releases
TTS_MODEL_NAME=auto
TTS_USE_GPU=auto

# Speech-to-Text (Vosk) - Lightweight, CPU-only
# STT_MODEL options: small (15MB), medium (128MB), large (1.8GB)
# Auto-downloads on first use
# STT_MODEL=small

# Re-Ranking (Phase 3) - Cross-encoder for quality boost
RERANK_ENABLED=0  # Set to 1 to enable (Week 2 of rollout)
RERANK_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2
RERANK_DEVICE=cpu  # or cuda
RERANK_TOP_K=50  # Retrieve 50 candidates for re-ranking

# Query Expansion (Phase 3) - Synonym/multi-query generation
QUERY_EXPANSION_ENABLED=0  # Set to 1 to enable (Week 3 of rollout)
EXPANSION_STRATEGY=synonyms  # synonyms or multi_query
EXPANSION_MAX_VARIANTS=5

# Connection Pooling (Phase 2) - High throughput
QDRANT_POOL_SIZE=3  # Number of Qdrant connections
QDRANT_POOL_TIMEOUT=30.0
QDRANT_POOL_RETRY_ENABLED=1
QDRANT_POOL_MAX_RETRIES=3
QDRANT_POOL_RETRY_DELAY=1.0

# ----------------------------------------------------------------------------
# Optional Monitoring (ALL FREE, RUNS LOCALLY)
# ----------------------------------------------------------------------------
# Uncomment to enable Prometheus + Grafana
# GRAFANA_PASSWORD=admin

# ----------------------------------------------------------------------------
# Deployment Profiles (Uncomment one section based on your needs)
# ----------------------------------------------------------------------------

# ==========================
# PROFILE: Development (Fast iteration, minimal resources)
# ==========================
# EMBEDDING_MODEL=all-MiniLM-L6-v2
# EMBEDDING_DEVICE=cpu
# RERANK_ENABLED=0
# QUERY_EXPANSION_ENABLED=0
# QDRANT_POOL_SIZE=1

# ==========================
# PROFILE: Staging (Balanced, test quality improvements)
# ==========================
# EMBEDDING_MODEL=all-MiniLM-L6-v2
# EMBEDDING_DEVICE=cpu
# RERANK_ENABLED=1
# RERANK_TOP_K=30
# QUERY_EXPANSION_ENABLED=1
# EXPANSION_STRATEGY=synonyms
# QDRANT_POOL_SIZE=3

# ==========================
# PROFILE: Production (High Quality, requires more RAM/CPU)
# ==========================
# EMBEDDING_MODEL=all-mpnet-base-v2
# EMBEDDING_DEVICE=cuda  # Requires NVIDIA GPU
# RERANK_ENABLED=1
# RERANK_MODEL=ms-marco-electra-base
# RERANK_DEVICE=cuda
# RERANK_TOP_K=50
# QUERY_EXPANSION_ENABLED=1
# EXPANSION_STRATEGY=multi_query
# QDRANT_POOL_SIZE=10

# ----------------------------------------------------------------------------
# Home Server Notes
# ----------------------------------------------------------------------------
# This entire stack runs locally on your home server with ZERO cloud costs!
# - All Docker images: FREE
# - All ML models: FREE (auto-downloaded from HuggingFace)
# - Qdrant vector DB: FREE (open source)
# - NATS event bus: FREE (optional)
# - Prometheus + Grafana: FREE (optional monitoring)
#
# Minimum Requirements: 4-core CPU, 8GB RAM, 50GB disk
# Recommended: 8-core CPU, 16GB RAM, 100GB SSD, optional GPU
#
# Total Monthly Cost: $0 (just electricity!)
# ----------------------------------------------------------------------------
# ============================================================================
# Task 4 - Dashboard Configuration & WebSocket Support
# ============================================================================

# ─────────────────────────────────────────────────────────────────────────────
# Dashboard Configuration
# ─────────────────────────────────────────────────────────────────────────────
DASHBOARD_HOST=0.0.0.0
DASHBOARD_PORT=8080
DASHBOARD_ENV=development

# ─────────────────────────────────────────────────────────────────────────────
# WebSocket Configuration (Real-time Updates)
# ─────────────────────────────────────────────────────────────────────────────
WEBSOCKET_HOST=localhost
WEBSOCKET_PORT=8000
WEBSOCKET_PROTOCOL=ws        # ws or wss for secure

# ─────────────────────────────────────────────────────────────────────────────
# Monitoring Features (System & GPU)
# ─────────────────────────────────────────────────────────────────────────────
# System Monitoring (via psutil) - CPU, RAM, disk, network
FEATURE_SYSTEM_MONITORING=true

# GPU Monitoring (via GPUtil) - requires NVIDIA GPU with drivers
ENABLE_GPU_MONITORING=false
FEATURE_GPU_MONITORING=false

# Temperature Monitoring - requires lm-sensors on Linux
ENABLE_TEMPERATURE_MONITORING=false
FEATURE_TEMPERATURE_MONITORING=false

# ─────────────────────────────────────────────────────────────────────────────
# Feature Flags (Dashboard Panels & Real-time Updates)
# ─────────────────────────────────────────────────────────────────────────────
# Real-time Updates
FEATURE_REAL_TIME_UPDATES=true
FEATURE_WEBSOCKET_FALLBACK=true

# Dashboard Panels
FEATURE_GOVERNANCE_PANEL=true
FEATURE_AI_SYSTEM_PANEL=true
FEATURE_INTELLIGENCE_ARENA=true
FEATURE_OMNI_MONITOR=true
FEATURE_CHAT_OPTIMIZATION=true
FEATURE_DATABASE_MONITORING=true

# Chat Configuration
DEBUG_CHAT_MODE=false

# Advanced Features
FEATURE_DEBUG_MODE=false
FEATURE_PERFORMANCE_METRICS=false
FEATURE_ERROR_LOGGING=true
COLLECT_METRICS=false

# UI Configuration
SHOW_GPU_METRICS=false