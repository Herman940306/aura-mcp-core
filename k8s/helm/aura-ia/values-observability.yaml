# =============================================================================
# Aura IA Observability Stack - Helm Values
# Deploy Prometheus, Grafana, Loki, Tempo, and OTel Collector
# =============================================================================

# -----------------------------------------------------------------------------
# Prometheus
# -----------------------------------------------------------------------------
prometheus:
  enabled: true

  server:
    image:
      repository: prom/prometheus
      tag: v2.48.0

    persistentVolume:
      enabled: true
      size: 20Gi

    retention: "15d"

    resources:
      requests:
        cpu: "200m"
        memory: "512Mi"
      limits:
        cpu: "1000m"
        memory: "2Gi"

    # Scrape configs for Aura IA services
    extraScrapeConfigs:
      - job_name: 'aura-ia-gateway'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names: ['aura-ia']
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app]
            action: keep
            regex: aura-ia-gateway
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod

      - job_name: 'aura-ia-ml'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names: ['aura-ia']
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app]
            action: keep
            regex: aura-ia-ml
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod

      - job_name: 'aura-ia-rag'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names: ['aura-ia']
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app]
            action: keep
            regex: aura-ia-rag
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod

      - job_name: 'aura-ia-role-engine'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names: ['aura-ia']
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app]
            action: keep
            regex: aura-ia-role-engine
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod

  # Alert rules
  serverFiles:
    alerting_rules.yml:
      groups:
        - name: aura-ia-alerts
          interval: 30s
          rules:
            - alert: AuraHighErrorRate
              expr: |
                sum(rate(http_requests_total{namespace="aura-ia",status=~"5.."}[5m]))
                / sum(rate(http_requests_total{namespace="aura-ia"}[5m])) > 0.01
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: "High error rate in Aura IA"
                description: "Error rate is {{ $value | printf \"%.2f\" }}% (>1%)"

            - alert: AuraHighLatency
              expr: |
                histogram_quantile(0.99,
                  sum(rate(http_request_duration_seconds_bucket{namespace="aura-ia"}[5m])) by (le, service)
                ) > 2
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "High P99 latency in {{ $labels.service }}"
                description: "P99 latency is {{ $value | printf \"%.2f\" }}s (>2s)"

            - alert: AuraPodRestarting
              expr: increase(kube_pod_container_status_restarts_total{namespace="aura-ia"}[1h]) > 3
              for: 10m
              labels:
                severity: warning
              annotations:
                summary: "Pod {{ $labels.pod }} restarting frequently"
                description: "Pod has restarted {{ $value }} times in the last hour"

            - alert: AuraGatewayDown
              expr: up{job="aura-ia-gateway"} == 0
              for: 2m
              labels:
                severity: critical
              annotations:
                summary: "Aura IA Gateway is down"
                description: "Gateway has been unavailable for more than 2 minutes"

            - alert: AuraMLQueueHigh
              expr: aura_ml_queue_depth > 100
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "ML Backend queue is filling up"
                description: "Queue depth is {{ $value }} (>100)"

            - alert: AuraMemoryPressure
              expr: |
                container_memory_usage_bytes{namespace="aura-ia"}
                / container_spec_memory_limit_bytes > 0.85
              for: 10m
              labels:
                severity: warning
              annotations:
                summary: "High memory usage in {{ $labels.container }}"
                description: "Memory usage is {{ $value | printf \"%.0f\" }}% of limit"

    recording_rules.yml:
      groups:
        - name: aura-ia-recording
          interval: 30s
          rules:
            - record: aura:http_requests:rate5m
              expr: sum(rate(http_requests_total{namespace="aura-ia"}[5m])) by (service, method, status)

            - record: aura:http_errors:rate5m
              expr: sum(rate(http_requests_total{namespace="aura-ia",status=~"5.."}[5m])) by (service)

            - record: aura:http_latency:p50
              expr: histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket{namespace="aura-ia"}[5m])) by (le, service))

            - record: aura:http_latency:p95
              expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{namespace="aura-ia"}[5m])) by (le, service))

            - record: aura:http_latency:p99
              expr: histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket{namespace="aura-ia"}[5m])) by (le, service))

  alertmanager:
    enabled: true

    config:
      global:
        resolve_timeout: 5m

      route:
        group_by: ['alertname', 'severity']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 4h
        receiver: 'default'
        routes:
          - match:
              severity: critical
            receiver: 'critical'
          - match:
              severity: warning
            receiver: 'warning'

      receivers:
        - name: 'default'
          webhook_configs:
            - url: 'http://alertmanager-webhook:9095/hook'
              send_resolved: true

        - name: 'critical'
          webhook_configs:
            - url: 'http://alertmanager-webhook:9095/hook/critical'
              send_resolved: true

        - name: 'warning'
          webhook_configs:
            - url: 'http://alertmanager-webhook:9095/hook/warning'
              send_resolved: true

# -----------------------------------------------------------------------------
# Grafana
# -----------------------------------------------------------------------------
grafana:
  enabled: true

  image:
    repository: grafana/grafana
    tag: 10.2.2

  adminUser: admin
  adminPassword: aura-ia-grafana  # Change in production!

  persistence:
    enabled: true
    size: 5Gi

  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "512Mi"

  # Datasources
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          url: http://prometheus-server:9090
          access: proxy
          isDefault: true
          jsonData:
            timeInterval: "15s"

        - name: Loki
          type: loki
          url: http://loki:3100
          access: proxy
          jsonData:
            maxLines: 1000

        - name: Tempo
          type: tempo
          url: http://tempo:3200
          access: proxy
          jsonData:
            httpMethod: GET
            tracesToLogs:
              datasourceUid: loki
              tags: ['service']
              mappedTags: [{ key: 'service.name', value: 'service' }]
              mapTagNamesEnabled: true
              spanStartTimeShift: '1h'
              spanEndTimeShift: '1h'
              filterByTraceID: true
              filterBySpanID: true

  # Dashboard provisioning
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'aura-ia'
          orgId: 1
          folder: 'Aura IA'
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/aura-ia

  # Import dashboards from ConfigMap
  dashboardsConfigMaps:
    aura-ia: "grafana-dashboards-aura-ia"

  # Grafana configuration
  grafana.ini:
    server:
      root_url: "%(protocol)s://%(domain)s:%(http_port)s/grafana/"

    security:
      admin_password: "${GF_SECURITY_ADMIN_PASSWORD}"

    users:
      allow_sign_up: false

    auth.anonymous:
      enabled: false

    alerting:
      enabled: true
      execute_alerts: true

    unified_alerting:
      enabled: true

# -----------------------------------------------------------------------------
# Loki
# -----------------------------------------------------------------------------
loki:
  enabled: true

  image:
    repository: grafana/loki
    tag: 2.9.2

  persistence:
    enabled: true
    size: 20Gi

  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "1Gi"

  config:
    auth_enabled: false

    server:
      http_listen_port: 3100
      grpc_listen_port: 9096

    common:
      path_prefix: /loki
      storage:
        filesystem:
          chunks_directory: /loki/chunks
          rules_directory: /loki/rules
      replication_factor: 1
      ring:
        instance_addr: 127.0.0.1
        kvstore:
          store: inmemory

    schema_config:
      configs:
        - from: 2020-10-24
          store: boltdb-shipper
          object_store: filesystem
          schema: v11
          index:
            prefix: index_
            period: 24h

    storage_config:
      boltdb_shipper:
        active_index_directory: /loki/boltdb-shipper-active
        cache_location: /loki/boltdb-shipper-cache
        cache_ttl: 24h
        shared_store: filesystem
      filesystem:
        directory: /loki/chunks

    limits_config:
      enforce_metric_name: false
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      max_entries_limit_per_query: 5000

    chunk_store_config:
      max_look_back_period: 0s

    table_manager:
      retention_deletes_enabled: true
      retention_period: 336h  # 14 days

    ruler:
      storage:
        type: local
        local:
          directory: /loki/rules
      rule_path: /loki/rules-temp
      alertmanager_url: http://prometheus-alertmanager:9093
      ring:
        kvstore:
          store: inmemory
      enable_api: true

# -----------------------------------------------------------------------------
# Tempo (Distributed Tracing)
# -----------------------------------------------------------------------------
tempo:
  enabled: true

  image:
    repository: grafana/tempo
    tag: 2.3.1

  persistence:
    enabled: true
    size: 10Gi

  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "1Gi"

  config:
    server:
      http_listen_port: 3200
      grpc_listen_port: 9095

    distributor:
      receivers:
        jaeger:
          protocols:
            thrift_http:
              endpoint: 0.0.0.0:14268
            grpc:
              endpoint: 0.0.0.0:14250
        otlp:
          protocols:
            http:
              endpoint: 0.0.0.0:4318
            grpc:
              endpoint: 0.0.0.0:4317
        zipkin:
          endpoint: 0.0.0.0:9411

    ingester:
      trace_idle_period: 10s
      max_block_bytes: 1073741824
      max_block_duration: 5m

    compactor:
      compaction:
        compaction_window: 1h
        max_block_bytes: 107374182400
        block_retention: 48h
        compacted_block_retention: 1h

    storage:
      trace:
        backend: local
        local:
          path: /tmp/tempo/blocks
        wal:
          path: /tmp/tempo/wal
        pool:
          max_workers: 100
          queue_depth: 10000

# -----------------------------------------------------------------------------
# OpenTelemetry Collector
# -----------------------------------------------------------------------------
otelCollector:
  enabled: true

  image:
    repository: otel/opentelemetry-collector-contrib
    tag: 0.90.1

  mode: deployment  # or daemonset

  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "500m"
      memory: "512Mi"

  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

      prometheus:
        config:
          scrape_configs:
            - job_name: 'otel-collector'
              scrape_interval: 10s
              static_configs:
                - targets: ['localhost:8888']

    processors:
      batch:
        send_batch_size: 1000
        timeout: 10s

      memory_limiter:
        check_interval: 1s
        limit_percentage: 80
        spike_limit_percentage: 25

      resource:
        attributes:
          - key: service.namespace
            value: aura-ia
            action: upsert

    exporters:
      otlp/tempo:
        endpoint: tempo:4317
        tls:
          insecure: true

      prometheus:
        endpoint: 0.0.0.0:8889
        namespace: aura_ia
        const_labels:
          source: otel

      loki:
        endpoint: http://loki:3100/loki/api/v1/push
        labels:
          attributes:
            service.name: "service"
            service.namespace: "namespace"
          resource:
            deployment.environment: "environment"

    extensions:
      health_check:
        endpoint: 0.0.0.0:13133
      pprof:
        endpoint: 0.0.0.0:1777
      zpages:
        endpoint: 0.0.0.0:55679

    service:
      extensions: [health_check, pprof, zpages]
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch, resource]
          exporters: [otlp/tempo]

        metrics:
          receivers: [otlp, prometheus]
          processors: [memory_limiter, batch, resource]
          exporters: [prometheus]

        logs:
          receivers: [otlp]
          processors: [memory_limiter, batch, resource]
          exporters: [loki]

# -----------------------------------------------------------------------------
# ServiceMonitor for Prometheus Operator (if using kube-prometheus-stack)
# -----------------------------------------------------------------------------
serviceMonitors:
  enabled: true

  gateway:
    enabled: true
    endpoints:
      - port: metrics
        path: /metrics
        interval: 15s

  ml:
    enabled: true
    endpoints:
      - port: metrics
        path: /metrics
        interval: 15s

  rag:
    enabled: true
    endpoints:
      - port: metrics
        path: /metrics
        interval: 15s

  roleEngine:
    enabled: true
    endpoints:
      - port: metrics
        path: /metrics
        interval: 15s

# -----------------------------------------------------------------------------
# Global Settings
# -----------------------------------------------------------------------------
global:
  namespace: aura-ia-monitoring

  # Storage class for all PVCs
  storageClass: ""

  # Image pull secrets
  imagePullSecrets: []
