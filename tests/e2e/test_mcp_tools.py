"""
MCP Tool Tests - Comprehensive testing for all MCP tools
Aura IA MCP E2E Test Suite

Tests:
- Tool registration and discovery
- Input schema enforcement
- Error handling
- Tool execution
- Response validation
- Streaming/long-running tasks
"""

from __future__ import annotations

import json
from datetime import datetime
from pathlib import Path

import httpx
import pytest
from model_output_validator import TOOL_RESULTS_DIR, ModelOutputValidator

# Service URLs
ML_BACKEND_URL = "http://localhost:9201"
GATEWAY_URL = "http://localhost:9200"

# Evidence directory
EVIDENCE_DIR = Path(__file__).parent.parent.parent / "e2e-evidence"
EVIDENCE_DIR.mkdir(parents=True, exist_ok=True)


def save_tool_evidence(
    tool_name: str, request: dict, response: dict, status: str
) -> Path:
    """Save tool test evidence to file."""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    evidence = {
        "tool_name": tool_name,
        "timestamp": datetime.now().isoformat(),
        "status": status,
        "request": request,
        "response": response,
    }
    filepath = TOOL_RESULTS_DIR / f"{timestamp}_{tool_name}_{status}.json"
    with open(filepath, "w") as f:
        json.dump(evidence, f, indent=2, default=str)
    return filepath


class TestMCPToolDiscovery:
    """Test MCP tool registration and discovery."""

    @pytest.fixture(autouse=True)
    def setup(self):
        """Setup validator for each test."""
        self.validator = ModelOutputValidator()
        self.client = httpx.Client(timeout=30.0)

    def teardown_method(self):
        """Cleanup after each test."""
        self.client.close()

    def test_backend_health(self):
        """Backend should be healthy before tool tests."""
        response = self.client.get(f"{ML_BACKEND_URL}/health")
        assert (
            response.status_code == 200
        ), f"Backend unhealthy: {response.text}"

        data = response.json()
        save_tool_evidence("health_check", {}, data, "passed")

        assert (
            "status" in data
            or "backend" in data
            or response.status_code == 200
        )

    def test_chat_status_returns_tool_info(self):
        """Chat status should return tool availability info."""
        response = self.client.post(
            f"{ML_BACKEND_URL}/chat/status",
            json={},
        )

        if response.status_code == 200:
            data = response.json()
            save_tool_evidence("chat_status", {}, data, "passed")

            # Should have tool-related info
            assert isinstance(data, dict)
        else:
            # Endpoint may not exist - mark as skipped
            pytest.skip("Chat status endpoint not available")


class TestMCPChatTools:
    """Test chat-integrated MCP tools."""

    @pytest.fixture(autouse=True)
    def setup(self):
        """Setup for each test."""
        self.validator = ModelOutputValidator()
        self.client = httpx.Client(timeout=120.0)  # Long timeout for LLM

    def teardown_method(self):
        """Cleanup after each test."""
        self.client.close()

    def test_weather_tool_execution(self):
        """Weather tool should return formatted weather data."""
        request_data = {
            "message": "what's the weather in Brackenfell",
            "conversation_id": "e2e-test-weather",
            "mode": "general",
        }

        response = self.client.post(
            f"{ML_BACKEND_URL}/chat/send",
            json=request_data,
        )

        assert response.status_code == 200, f"Chat failed: {response.text}"

        data = response.json()
        save_tool_evidence(
            "weather_tool",
            request_data,
            data,
            "passed" if data.get("success") else "failed",
        )

        # Validate response structure
        struct_result = self.validator.validate_chat_response_structure(data)
        assert struct_result.passed, struct_result.message

        # Validate weather content
        weather_result = self.validator.validate_weather_response(data)
        assert weather_result.passed, weather_result.message

        # Check for natural language response
        inner_response = data.get("response", {})
        if isinstance(inner_response, dict) and "response" in inner_response:
            text_response = inner_response["response"]
            assert (
                "¬∞C" in text_response or "¬∞F" in text_response
            ), "Weather response should include temperature unit"

    def test_weather_tool_different_locations(self):
        """Weather tool should work for different locations."""
        locations = ["Johannesburg", "Cape Town", "Durban"]

        for location in locations:
            request_data = {
                "message": f"what's the weather in {location}",
                "conversation_id": f"e2e-test-weather-{location.lower()}",
            }

            response = self.client.post(
                f"{ML_BACKEND_URL}/chat/send",
                json=request_data,
            )

            if response.status_code == 200:
                data = response.json()
                save_tool_evidence(
                    f"weather_{location.lower()}", request_data, data, "passed"
                )

                # Should have weather data
                assert (
                    data.get("success") is not False
                ), f"Weather failed for {location}"

    def test_time_tool_execution(self):
        """Time tool should return current time."""
        request_data = {
            "message": "what time is it",
            "conversation_id": "e2e-test-time",
        }

        response = self.client.post(
            f"{ML_BACKEND_URL}/chat/send",
            json=request_data,
        )

        assert response.status_code == 200
        data = response.json()
        save_tool_evidence(
            "time_tool",
            request_data,
            data,
            "passed" if data.get("success") else "failed",
        )

        # Validate structure
        result = self.validator.validate_chat_response_structure(data)
        assert result.passed, result.message

    def test_date_tool_execution(self):
        """Date tool should return current date."""
        request_data = {
            "message": "what's today's date",
            "conversation_id": "e2e-test-date",
        }

        response = self.client.post(
            f"{ML_BACKEND_URL}/chat/send",
            json=request_data,
        )

        assert response.status_code == 200
        data = response.json()
        save_tool_evidence(
            "date_tool",
            request_data,
            data,
            "passed" if data.get("success") else "failed",
        )

    def test_general_chat_without_tools(self):
        """General chat should work without triggering tools."""
        request_data = {
            "message": "Hello, how are you?",
            "conversation_id": "e2e-test-general",
        }

        response = self.client.post(
            f"{ML_BACKEND_URL}/chat/send",
            json=request_data,
        )

        assert response.status_code == 200
        data = response.json()
        save_tool_evidence("general_chat", request_data, data, "passed")

        # Should have a response
        assert "response" in data

        # Validate LLM response quality if string
        if isinstance(data.get("response"), str):
            quality_result = self.validator.validate_llm_response_quality(
                data["response"]
            )
            # Log but don't fail on quality - LLM responses vary


class TestMCPToolErrorHandling:
    """Test MCP tool error handling and edge cases."""

    @pytest.fixture(autouse=True)
    def setup(self):
        """Setup for each test."""
        self.validator = ModelOutputValidator()
        self.client = httpx.Client(timeout=30.0)

    def teardown_method(self):
        """Cleanup after each test."""
        self.client.close()

    def test_empty_message_handling(self):
        """Empty message should be handled gracefully."""
        request_data = {
            "message": "",
            "conversation_id": "e2e-test-empty",
        }

        response = self.client.post(
            f"{ML_BACKEND_URL}/chat/send",
            json=request_data,
        )

        # Should not crash - either 200 with error or 400
        assert response.status_code in [
            200,
            400,
            422,
        ], f"Unexpected status: {response.status_code}"

        data = (
            response.json()
            if response.status_code != 422
            else {"error": "validation_error"}
        )
        save_tool_evidence("empty_message", request_data, data, "handled")

    def test_very_long_message_handling(self):
        """Very long message should be handled gracefully."""
        request_data = {
            "message": "test " * 1000,  # Very long message
            "conversation_id": "e2e-test-long",
        }

        response = self.client.post(
            f"{ML_BACKEND_URL}/chat/send",
            json=request_data,
        )

        # Should handle without crashing
        assert response.status_code in [200, 400, 413, 422]
        save_tool_evidence(
            "long_message",
            {"message_length": len(request_data["message"])},
            {"status_code": response.status_code},
            "handled",
        )

    def test_invalid_location_weather(self):
        """Weather for invalid location should be handled."""
        request_data = {
            "message": "what's the weather in InvalidLocationXYZ123",
            "conversation_id": "e2e-test-invalid-location",
        }

        response = self.client.post(
            f"{ML_BACKEND_URL}/chat/send",
            json=request_data,
        )

        assert response.status_code == 200
        data = response.json()
        save_tool_evidence("invalid_location", request_data, data, "handled")

        # Should have some response (might be error or fallback)
        assert "response" in data or "error" in data

    def test_special_characters_in_message(self):
        """Messages with special characters should be handled."""
        request_data = {
            "message": "Hello! What's the weather? <script>alert('test')</script> üå§Ô∏è",
            "conversation_id": "e2e-test-special-chars",
        }

        response = self.client.post(
            f"{ML_BACKEND_URL}/chat/send",
            json=request_data,
        )

        # Should handle without crashing (XSS should be sanitized)
        assert response.status_code in [200, 400]
        save_tool_evidence(
            "special_chars",
            request_data,
            {"status_code": response.status_code},
            "handled",
        )


class TestMCPToolInputValidation:
    """Test MCP tool input schema enforcement."""

    @pytest.fixture(autouse=True)
    def setup(self):
        """Setup for each test."""
        self.client = httpx.Client(timeout=30.0)

    def teardown_method(self):
        """Cleanup after each test."""
        self.client.close()

    def test_missing_required_fields(self):
        """Missing required fields should return validation error."""
        # Send without message field
        response = self.client.post(
            f"{ML_BACKEND_URL}/chat/send",
            json={"conversation_id": "test"},
        )

        # Should return 422 (validation error) or handle gracefully
        assert response.status_code in [200, 400, 422]
        save_tool_evidence(
            "missing_fields",
            {"sent": {"conversation_id": "test"}},
            {"status_code": response.status_code},
            "handled",
        )

    def test_wrong_type_for_message(self):
        """Wrong type for message should be handled."""
        response = self.client.post(
            f"{ML_BACKEND_URL}/chat/send",
            json={
                "message": 12345,
                "conversation_id": "test",
            },  # Number instead of string
        )

        # 500 is acceptable - backend may crash on unexpected input types
        assert response.status_code in [200, 400, 422, 500]
        save_tool_evidence(
            "wrong_type",
            {"message_type": "int"},
            {"status_code": response.status_code},
            "handled",
        )

    def test_null_values(self):
        """Null values should be handled gracefully."""
        response = self.client.post(
            f"{ML_BACKEND_URL}/chat/send",
            json={"message": None, "conversation_id": None},
        )

        assert response.status_code in [200, 400, 422]
        save_tool_evidence(
            "null_values",
            {"message": None},
            {"status_code": response.status_code},
            "handled",
        )


class TestMCPToolConcurrency:
    """Test MCP tool behavior under concurrent requests."""

    @pytest.fixture(autouse=True)
    def setup(self):
        """Setup for each test."""
        self.validator = ModelOutputValidator()

    def test_concurrent_weather_requests(self):
        """Multiple concurrent weather requests should all succeed."""
        from concurrent.futures import ThreadPoolExecutor, as_completed

        def make_request(location: str) -> dict:
            with httpx.Client(timeout=120.0) as client:
                response = client.post(
                    f"{ML_BACKEND_URL}/chat/send",
                    json={
                        "message": f"what's the weather in {location}",
                        "conversation_id": f"e2e-concurrent-{location}",
                    },
                )
                return {
                    "location": location,
                    "status": response.status_code,
                    "data": (
                        response.json()
                        if response.status_code == 200
                        else None
                    ),
                }

        locations = ["Johannesburg", "Cape Town", "Durban"]
        results = []

        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {
                executor.submit(make_request, loc): loc for loc in locations
            }
            for future in as_completed(futures):
                try:
                    results.append(future.result())
                except Exception as e:
                    results.append(
                        {"location": futures[future], "error": str(e)}
                    )

        # All should complete (not necessarily succeed due to rate limiting)
        success_count = sum(
            1
            for r in results
            if isinstance(r, dict) and r.get("status") == 200
        )
        save_tool_evidence(
            "concurrent_requests",
            {"count": len(results)},
            {"success_count": success_count, "results": str(results)},
            "completed",
        )

        # At least some should succeed
        assert (
            success_count >= 1
        ), "At least one concurrent request should succeed"


class TestMCPToolResponseFormat:
    """Test MCP tool response format consistency."""

    @pytest.fixture(autouse=True)
    def setup(self):
        """Setup for each test."""
        self.validator = ModelOutputValidator()
        self.client = httpx.Client(timeout=120.0)

    def teardown_method(self):
        """Cleanup after each test."""
        self.client.close()

    def test_response_has_consistent_structure(self):
        """All tool responses should have consistent structure."""
        messages = [
            "what's the weather in Cape Town",
            "what time is it",
            "hello",
        ]

        responses = []
        for msg in messages:
            response = self.client.post(
                f"{ML_BACKEND_URL}/chat/send",
                json={
                    "message": msg,
                    "conversation_id": f"e2e-format-{hash(msg)}",
                },
            )

            if response.status_code == 200:
                data = response.json()
                responses.append(data)

                # All should have these fields
                assert (
                    "response" in data
                ), f"Missing 'response' field for: {msg}"

        # Validate consistency
        if responses:
            common_keys = set.intersection(*[set(r.keys()) for r in responses])
            assert (
                "response" in common_keys
            ), "All responses should have 'response' field"

            save_tool_evidence(
                "response_consistency",
                {"messages": messages},
                {
                    "common_keys": list(common_keys),
                    "response_count": len(responses),
                },
                "passed",
            )

    def test_weather_response_has_natural_language(self):
        """Weather response should include human-readable text."""
        response = self.client.post(
            f"{ML_BACKEND_URL}/chat/send",
            json={
                "message": "what's the weather in Pretoria",
                "conversation_id": "e2e-natural-lang",
            },
        )

        assert response.status_code == 200
        data = response.json()

        # Extract text response
        inner = data.get("response", {})
        if isinstance(inner, dict):
            text = inner.get("response", "")
        else:
            text = str(inner)

        # Should be natural language, not raw JSON
        assert not text.startswith(
            "{"
        ), "Response should be natural language, not raw JSON"

        # Should have weather-related content
        weather_indicators = [
            "¬∞",
            "temperature",
            "weather",
            "wind",
            "cloud",
            "sun",
            "rain",
            "clear",
        ]
        has_weather = any(
            ind.lower() in text.lower() for ind in weather_indicators
        )
        assert (
            has_weather
        ), f"Response should contain weather information: {text[:200]}"

        save_tool_evidence(
            "natural_language",
            {"message": "weather in Pretoria"},
            {"response_text": text[:500]},
            "passed",
        )


# Generate test summary
@pytest.fixture(scope="session", autouse=True)
def generate_mcp_test_summary(request):
    """Generate summary after all MCP tests."""
    yield

    # Count evidence files
    tool_results = (
        list(TOOL_RESULTS_DIR.glob("*.json"))
        if TOOL_RESULTS_DIR.exists()
        else []
    )

    summary = {
        "test_suite": "MCP Tool Tests",
        "timestamp": datetime.now().isoformat(),
        "evidence_files": len(tool_results),
        "evidence_directory": str(TOOL_RESULTS_DIR),
    }

    summary_path = EVIDENCE_DIR / "mcp_tools_summary.json"
    with open(summary_path, "w") as f:
        json.dump(summary, f, indent=2)

    print(f"\nüìä MCP Tool Test Summary saved to: {summary_path}")
    print(f"üìÅ Evidence files generated: {len(tool_results)}")
